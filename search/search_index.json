{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to my blog.</p> <p>With this blog I intend to keep notes of the issues I have faced and if time permits provide detailed references and guides to new tools and technologies.  What you might find here are notes related to Distributed Computing and Cloud development.  Find here my contribution to the koalas library which was then merged into Spark</p> <p>You can get in touch with me on  </p> <p>I created this blog using Mkdocs.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#content","title":"Content","text":"<ul> <li>2023<ul> <li>Tkinter not working in wsl2</li> <li>Asking Notion AI to write a tutorial on K8s </li> </ul> </li> <li>2022<ul> <li>Process Anchore &amp; Trivy vulnerability outputs</li> </ul> </li> <li>2021:<ul> <li>GO KIT vs Mux vs net/http</li> <li>Moving Docker images without base image</li> </ul> </li> <li>2020<ul> <li>Setup SparkOperator and Minio in K8s</li> </ul> </li> <li>2018:<ul> <li>Testing ssh connectivity to cluster</li> <li>Unable to install R packages rgl &amp; qpcR in sparkR</li> </ul> </li> <li>2017:<ul> <li>Sqoop views using Netezza</li> </ul> </li> <li>2013:<ul> <li>Install RStudio in CentOS</li> <li>Create sudo user in CentOS</li> <li>Setup Hbase in Cloudera</li> </ul> </li> </ul>"},{"location":"blog/2013/10/01/","title":"Setup HBase for Cloudera","text":"<p>Follow these steps</p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#install-hbase-on-all-the-machines","title":"Install <code>hbase</code> on all the machines","text":"<pre><code>sudo yum install hbase\n</code></pre>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#install-hbase-master-and-zookeper-server-on-your-master-machine","title":"Install <code>hbase-master</code> and <code>zookeper-server</code> on your master machine","text":"<p><pre><code>sudo yum install zookeper-server\nsudo yum install hbase-master\n</code></pre> zookeeper-server automatically installs the base zookeper package also. For hbase to start it needs to have zookeeper </p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#install-hbase-region-server-and-zookeeper-in-all-your-slave-machines","title":"Install <code>hbase-region</code> server and zookeeper in all your slave machines","text":"<pre><code>sudo yum install zookeeper\nsudo yum install hbase-regionserver\n</code></pre>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#modifying-the-hbase-configuration","title":"Modifying the HBase Configuration","text":"<p>To enable pseudo-distributed mode, you must first make some configuration changes. Open <code>/etc/hbase/conf/hbase-site.xml</code> in your editor of choice, and insert the following XML properties between the <code>&lt;configuration&gt;</code> and <code>&lt;/configuration&gt;</code> tags. Be sure to replace <code>myhost</code> with the hostname of your HDFS NameNode (as specified by <code>fs.default.name</code> or <code>fs.defaultFS</code> in your <code>hadoop/conf/core-site.xml</code> file). You may also need to change the port number from the default <code>8020</code> to another value. <pre><code>&lt;property&gt;\n  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;hbase.rootdir&lt;/name&gt;\n  &lt;value&gt;hdfs://myhost:8020/hbase&lt;/value&gt;\n&lt;/property&gt;\n</code></pre></p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#configuring-for-distributed-operation","title":"Configuring for Distributed Operation","text":"<p>After you have decided which machines will run each process, you can edit the configuration so that the nodes may locate each other. In order to do so, you should make sure that the configuration files are synchronized across the cluster. Cloudera strongly recommends the use of a configuration management system to synchronize the configuration files, though you can use a simpler solution such as rsync to get started quickly.</p> <p>The only configuration change necessary to move from pseudo-distributed operation to fully-distributed operation is the addition of the ZooKeeper Quorum address in hbase-site.xml. Insert the following XML property to configure the nodes with the address of the node where the ZooKeeper quorum peer is running: <pre><code>&lt;property&gt;\n  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;\n  &lt;value&gt;mymasternode&lt;/value&gt;\n&lt;/property&gt;\n</code></pre></p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#creating-the-hbase-directory-in-hdfs","title":"Creating the <code>/hbase</code> Directory in HDFS","text":"<p>Before starting the HBase Master, you need to create the /hbase directory in HDFS. The HBase master runs as hbase:hbase so it does not have the required permissions to create a top level directory.</p> <p>To create the <code>/hbase</code> directory in HDFS: <pre><code>sudo -u hdfs hadoop fs -mkdir /hbase\nsudo -u hdfs hadoop fs -chown hbase /hbase\n</code></pre></p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#starting-the-zookeeper-server","title":"Starting the ZooKeeper Server","text":"<p>To start ZooKeeper after a fresh install: <pre><code>sudo service zookeeper-server init\nsudo service zookeeper-server start\n</code></pre></p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#starting-the-hbase-master","title":"Starting the HBase Master","text":"<p>On Red Hat and SLES systems (using .rpm packages) you can now start the HBase Master by using the included service script: <pre><code>sudo service hbase-master start\n</code></pre></p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#starting-the-hbase-region-server","title":"Starting the HBase Region Server:","text":"<pre><code>sudo service hbase-regionserver start\n</code></pre>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/01/#accessing-hbase-by-using-the-hbase-shell","title":"Accessing HBase by using the HBase Shell","text":"<p>After you have started HBase, you can access the database by using the HBase Shell: <pre><code>hbase shell\n</code></pre></p> <p>Hope this blog helped you in setting up HBase. Please let me know if you have any questions.</p> <p>For further reference : Cloudera blog</p>","tags":["HBase","Cloudera"]},{"location":"blog/2013/10/02/","title":"Create sudo user in CentOS","text":"<p>How to create a user in CentOS and give sudo access To create a user in CentOS follow these steps</p> <ol> <li> <p>You must be logged in as root to add a new user</p> </li> <li> <p>Issue the useradd command to create a locked account <pre><code>useradd &lt;username&gt;\n</code></pre></p> </li> <li> <p>Issue the passwd command to set the password of the newly created user <pre><code>passwd &lt;username&gt;\n</code></pre></p> </li> </ol> <p>This will you prompt you to enter the password of the newly created user</p> <ol> <li>To give the user sudo access you need to add  the user to wheel group </li> </ol> <p>To do this issue the command : <code>visudo</code></p> <p>To be able to add a user to the wheel group you must uncomment the line ie <pre><code>#%wheel   ALL=(ALL)   ALL\n</code></pre> to <pre><code>%wheel   ALL=(ALL)   ALL\n</code></pre></p> <ol> <li> <p>Adding the newly created user to wheel group <pre><code>usermod -G wheel  &lt;username&gt;\n</code></pre></p> </li> <li> <p>Finished  - Your user now has sudo access !! enjoy !!</p> </li> </ol>","tags":["CentOS","Linux"]},{"location":"blog/2013/10/31/","title":"Install RStudio in CentOS 6","text":"<p>Centos doesnot support R studio desktop so you would have to install r studio server instead which works in a browser </p>","tags":["RStudio","CentOS"]},{"location":"blog/2013/10/31/#for-el6","title":"For EL6","text":"<p>RStudio Server has several dependencies on packages (including R itself) found in the Extra Packages for Enterprise Linux (EPEL) repository. If you don't already have this repository available you should add it to your system using the instructions found on the Fedora EPEL website.</p> <pre><code>$ su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm'\n</code></pre> <p>After enabling EPEL you should then ensure that you have installed the version of R available from EPEL. You can do this using the following command:</p> <pre><code>$ sudo yum install R\n</code></pre>","tags":["RStudio","CentOS"]},{"location":"blog/2013/10/31/#download-and-install","title":"Download and Install","text":"<p>To download and install RStudio Server open a terminal window and execute the commands corresponding to the 32 or 64-bit version as appropriate. - 32-bit Size: 17.5 MB MD5: 3bc83db8c23c212c391342e731f65823 <pre><code>$ wget http://download2.rstudio.org/rstudio-server-0.97.551-i686.rpm \n$ sudo yum install --nogpgcheck rstudio-server-0.97.551-i686.rpm\n</code></pre></p> <ul> <li>64-bit Size: 17.6 MB MD5: c89d5574a587472d06f72b304356a776</li> </ul> <pre><code>$ wget http://download2.rstudio.org/rstudio-server-0.97.551-x86_64.rpm \n$ sudo yum install --nogpgcheck rstudio-server-0.97.551-x86_64.rpm\n</code></pre> <p>Then in your browser go to address <pre><code>http://&lt;your_server_name&gt;:8787\n</code></pre></p> <p>The login credentials are the current username and password of your centos account </p> <p>Reference link</p>","tags":["RStudio","CentOS"]},{"location":"blog/2017/07/18/","title":"Sqoop Views in Netezza to HDFS","text":"<p>I pondered upon a use case to transfer netezza tables/views to hadoop system. The current flow that we are using are : 1. Netezza -&gt; SAN 2. SAN -&gt; S3 3. S3 -&gt; hdfs</p> <p>If there is no primary key for the table in netezza you will be forced to use -split-by option or -m option. Only use verbose if needed.</p> <p>And the reverse to transfer to netezza. After analyzing the use case the best option i found was to use sqoop. We are using yarn queues hence ignore the <code>-Dmapreduce.job.queuename=q1</code> this option if none is setup.</p>","tags":["Bigdata","Apache Sqoop","Netezza"]},{"location":"blog/2017/07/18/#transfer-view","title":"Transfer view","text":"<p>Sqoop doesnot allow you to write into existing directory so removing the directory before transferring <pre><code>$ hdfs dfs -rm -R /apps/hive/warehouse/&lt;hivedbname&gt;.db/&lt;hivetablename&gt;\n\n$ sqoop import -Dmapreduce.job.queuename=q1 \n    --hive-import \n    --hive-database &lt;hivedbname&gt; \n    --hive-table &lt;hivetablename&gt; \n    --driver org.netezza.Driver \n    --direct \n    --connect jdbc:netezza://&lt;host&gt;:&lt;port&gt;/&lt;netezzadbname&gt; \n    --username &lt;netezzauser&gt; \n    --password &lt;netezzapwd&gt; \n    --table &lt;netezza tablename&gt; \n    --target-dir hdfs:///apps/hive/warehouse/&lt;hivedbname&gt;.db/&lt;hivetablename&gt; \n    -split-by &lt;anycolumn&gt;\n</code></pre></p> <p>If we dont use <code>--driver org.netezza.Driver</code> parameter the following error is encountered. <pre><code>2017-07-18 09:34:53,079 ERROR [Thread-16] org.apache.sqoop.mapreduce.db.netezza.NetezzaJDBCStatementRunner: Unable to execute external table export\norg.netezza.error.NzSQLException: ERROR:  Column reference \"DATASLICEID\" not supported for views\n at org.netezza.internal.QueryExecutor.getNextResult(QueryExecutor.java:276)\n at org.netezza.internal.QueryExecutor.execute(QueryExecutor.java:73)\n at org.netezza.sql.NzConnection.execute(NzConnection.java:2673)\n at org.netezza.sql.NzStatement._execute(NzStatement.java:849)\n at org.netezza.sql.NzPreparedStatament.execute(NzPreparedStatament.java:152)\n at org.apache.sqoop.mapreduce.db.netezza.NetezzaJDBCStatementRunner.run(NetezzaJDBCStatementRunner.java:75)\n\nEnd of LogType:syslog\n</code></pre></p> <p>Instead of split by option we can also use -m 1 , which transfers the data in one mapper &amp; can be a bit slow.</p>","tags":["Bigdata","Apache Sqoop","Netezza"]},{"location":"blog/2017/07/18/#transfer-a-table","title":"Transfer a table","text":"<p>Sqoop doesnot allow you to write into existing directory so removing the directory before transferring <pre><code>$ hdfs dfs -rm -R /apps/hive/warehouse/&lt;hivedbname&gt;.db/&lt;hivetablename&gt;\n\n$ sqoop import \n    -Dmapreduce.job.queuename=q1 \n    --verbose \n    --hive-import \n    --hive-database jijo \n    --direct \n    --connect jdbc:netezza://&lt;host&gt;:&lt;port&gt;/&lt;netezzadbname&gt; \n    --username &lt;netezzauser&gt; \n    --password &lt;netezzapwd&gt; \n    --table &lt;netezza tablename&gt;\n    --target-dir hdfs:///apps/hive/warehouse/&lt;hivedbname&gt;.db/&lt;hivetablename&gt; \n    -m 1\n</code></pre></p> <p>Running <code>analyze table &lt;hivedbname&gt;.&lt;hivetablename&gt; compute statistics</code> would be ideal for hive running on tez execution engine.</p>","tags":["Bigdata","Apache Sqoop","Netezza"]},{"location":"blog/2018/02/multiple-ssh-test/","title":"Testing ssh connectivity to cluster","text":"<p>We often encounter infrastructure issues when we start our processing. I had faced this issue often enough that made me do something about it. I thought of testing the connectivity through python since most of our code was on python and it was easier to integrate into the stack.</p> <p>I researched about many testing libraries and pytest stood out. Paramiko is another library to test ssh connectivity. Combining both of these to analyze whether our infrastructure is ready or not.</p> <p>Use case :  check ssh connectivity to multiple clusters in one test</p> <pre><code>#!/usr/bin/python\nimport paramiko\nimport pytest\n\n#define server ips to which connectivity is to be tested\n#this method assumes that the username and the key file are same \n#across all the servers\n@pytest.fixture(params=[\"ip1.ip1.ip1.ip1\", \"ip2.ip2.ip2.ip2\"])\ndef ssh(request):\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(request.param,username=\"username\",key_filename=\"connect.pem\")\n    yield ssh\n    ssh.close()\n\n# test connectivity by echoing hello and if the program is able to read\n# the output from the stdout\ndef test_hello(ssh):\n    stdin, stdout, stderr = ssh.exec_command(\"echo hello\")\n    stdin.close()\n    stderr.read() == b\"\"\n    assert stdout.read() == b\"hello\\n\"\n</code></pre> <p>Run <code>pytest</code> in the terminal to test this.</p> <p>To check plan of pytest run : <pre><code>pytest --collect-only\n</code></pre></p> <p>Fixture does the setup of environment before we execute our test cases , in this case since there are multiple parameters , you can find more than one call for that method in the test plan.</p> <p>To use a fixture in our method, just pass the method name in the function arguments <pre><code>$ pytest --collect-only\ncollected 2 items \n&lt;Module Utility_test.py'&gt;\n  &lt;Function 'test_hello[10.30.107.85]'&gt;\n  &lt;Function 'test_hello[10.20.91.148]'&gt;\n</code></pre></p>"},{"location":"blog/2018/02/r-rgl-qpcr-sparkr/","title":"Unable to install R packages rgl & qpcR in sparkR","text":"<p>If you encounter such an error while installing the package rgl in sparkR <pre><code>configure: using libpng dynamic linkage checking for X... no \nconfigure: error: X11 not found but required, \nconfigure aborted.\n</code></pre></p> <p>its because X11 is a windows library and to resolve use:</p> <p>Ubuntu: <pre><code>sudo apt-get install libglu1-mesa-dev\n</code></pre></p> <p>Redhat: <pre><code>sudo yum install mesa-libGL-devel mesa-libGLU-devel libpng-devel\n</code></pre></p>"},{"location":"blog/2020/01/19/","title":"Setup SparkOperator and Minio in K8s","text":"<p>This post assumes that you already have a running k8s cluster setup as minikube or in docker-desktop and you are familiar with kubectl commands.</p>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#prerequisites","title":"Prerequisites","text":"<ul> <li>minikube or any other k8s environment</li> <li>kubectl</li> <li>helm</li> </ul>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#install-spark-operator","title":"Install spark operator","text":"","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#create-namespaces","title":"Create Namespaces","text":"<pre><code>kubectl create ns spark-operator\nkubectl create ns spark-jobs\n</code></pre>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#add-helm-repo","title":"Add Helm repo","text":"<pre><code>helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator\n</code></pre>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#install-helm-chart-for-spark","title":"Install Helm chart for Spark","text":"<p>Webhook must be enabled for mounting volumes to driver and executor nodes and also for environment variables defined on pods to work. <pre><code>helm install my-spark spark-operator/spark-operator --namespace spark-operator --set sparkJobNamespace=spark-jobs --set webhook.enable=true --set webhook.port=443\n</code></pre></p>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#install-minio","title":"Install Minio","text":"","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#create-namespaces_1","title":"Create Namespaces","text":"<pre><code>kubectl create ns minio\n</code></pre>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#add-helm-repo_1","title":"Add Helm repo","text":"<pre><code>helm repo add minio https://helm.min.io/\n</code></pre>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#install-helm-chart-for-minio","title":"Install Helm chart for  Minio","text":"<pre><code>helm install --namespace minio --generate-name minio/minio\n</code></pre>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#setup-minio","title":"Setup Minio","text":"<p>Minio will be installed with its access key &amp; secret key which should be used while connecting to the service There is a separate image for minio cli minio/mc which should be used to upload your data to minio or it can be accessed using UI also for which we should do a port forward of the minio service.</p>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2020/01/19/#move-data-to-minio","title":"Move data to minio","text":"<p><pre><code>## Setup minio credentials\n1. export MINIO_ACCESSKEY=`kubectl get secret -n minio -l app=minio -o json | jq .data.accesskey | sed s/\\\"//g | base64 -d`\n2. export MINIO_SECRETKEY=`kubectl get secret -n minio -l app=minio -o json | jq .data.secretkey | sed s/\\\"//g | base64 -d`\n3. export MINIO_HOST=`kubectl get svc -n minio -l app=minio -ojsonpath='http://{.items[0].metadata.name}:{.items[0].spec.ports[0].targetPort}'`\n4. kubectl run -n minio -it --rm minio-cli --env MINIO_HOST=$MINIO_HOST --env MINIO_ACCESSKEY=$MINIO_ACCESSKEY --env MINIO_SECRETKEY=$MINIO_SECRETKEY \\\n   --image=minio/mc --command sh\n</code></pre> Run the following commands inside the container in step 4 : <pre><code>mc config host add myminio ${MINIO_HOST} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY}\nmc ls myminio\n</code></pre> Use <code>mc cp</code> command to copy data from the container to minio.</p> <p>Use <code>wget</code> commands to fetch data to container.</p>","tags":["Spark","SparkOperator","Kubernetes"]},{"location":"blog/2021/08/24/","title":"GO KIT vs Mux vs net/http","text":"","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#gokit","title":"GoKit","text":"<p>GoKit is more of do it urself thing where they define the flow and leave it to us to define the functions</p> <p>https://github.com/go-kit/kit/blob/master/transport/http/client.go#L90</p> <p>https://github.com/go-kit/kit/blob/master/transport/http/server.go#L95</p> <p>Comparison</p>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#server-side-client-side","title":"Server Side &amp; Client Side","text":"<p>Sever Side</p> <p>In server side for each endpoint we can write</p> <ol> <li>ServerBeforefns<ol> <li>typically used for getting request headers, token data etc and putting into context</li> </ol> </li> <li>Decode fn<ol> <li>request json to struct conversion</li> </ol> </li> <li>Enpoint fn <ol> <li>calls the service implementation</li> </ol> </li> <li>ServerAfter fn<ol> <li>for any cleanup work</li> </ol> </li> <li>Encode fn <ol> <li>response struct to json</li> </ol> </li> </ol> <p>Error Handling</p> <p>Anypoint if there is an error, we can also use an Error handler to handle the error. Basically do something like REST http code to service error code mapping. Its left upto us on how to define this</p> <p>Client Side</p> <ol> <li>Client gets inputs and makes a struct</li> <li>Which is encoded into json and sent as payload in req</li> <li>Similar to server side here also we have ClientBefore and ClientAfter fns</li> </ol>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#plugging-middlewares","title":"Plugging Middlewares","text":"<p>Since this only recommends a certain format of fns, and all of these fns actually take in a request and give a response.. they say we can plugin any sort of transport (http, grpc etc)... Havent explored to this level but the go-kit github has examples</p> <p>Also it is very easy to chain endpoint fn - mainly used for plugging middlewares like logging</p> <pre><code>typeloggingMiddlewarestruct\u00a0{\n    next\u00a0\u00a0\u00a0Service\n    logger\u00a0log.Logger\n}\n\nfunc\u00a0NewLoggingMiddleware(logger\u00a0log.Logger)\u00a0Middleware\u00a0{\n    returnfunc(next\u00a0Service)\u00a0Service\u00a0{\n        return\u00a0&amp;loggingMiddleware{\n            next:\u00a0\u00a0\u00a0next,\n            logger:\u00a0logger,\n        }\n    }\n}\n\nfunc\u00a0(mw\u00a0loggingMiddleware)\u00a0PostPackage(ctx\u00a0context.Context,\u00a0packageOptions\u00a0*PackageRequestMeta,\u00a0valuesFile\u00a0*fileUtils.InputFile,\u00a0archiveFile\u00a0*fileUtils.InputFile)\u00a0(application\u00a0*v1alpha1.Application,\u00a0err\u00a0error)\u00a0{\n    deferfunc(begin\u00a0time.Time)\u00a0{\n        mw.logger.Log(\"method\",\u00a0\"PostPackage\",\u00a0\"took\",\u00a0time.Since(begin),\u00a0\"err\",\u00a0err)\n    }(time.Now())\n    return\u00a0mw.next.PostPackage(ctx,\u00a0packageOptions,\u00a0valuesFile,\u00a0archiveFile)\n}\n\n// the NewLoggingMiddleware fn is like a higher order fn\n// I can pass any service to it ..\n\ndeployService\u00a0=\u00a0deploy.New(conf,\u00a0logger)\ndeployService\u00a0=\u00a0deploy.NewLoggingMiddleware(logger)(deployService)\n</code></pre> <p>It takes a while to get used to this but its very powerful.. I often get confused with all the abstractions and have to lookup stuff.</p> <p>In postPackage fn, it registers a deferred fn with parameter time.Now for begin then it calls the actual service implementation PostPackage. Once its executed and control returns here, it will log the execution time</p>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#constraints","title":"Constraints","text":"<p>Its hard to understand go/kit unless you run both client and server in debug mode in vscode and trace the steps</p>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#references","title":"References","text":"<ul> <li>Getting started with Go kit - M\u00e1rk S\u00e1gi-Kaz\u00e1r (sagikazarmark.hu)</li> <li>https://github.com/go-kit/kit/blob/master/transport/http/client.go#L90</li> <li>https://github.com/go-kit/kit/blob/master/transport/http/server.go#L95</li> </ul>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/08/24/#appendix","title":"Appendix","text":"<p>There are also ServerFinalizer and ClientFinalizer functions but I havent used them much</p>","tags":["Go","Gokit","Mux","net/http","golang"]},{"location":"blog/2021/09/08/","title":"Moving Docker images without base image","text":"<p>I was learning about the new buildkit features as well as researching a doubt which my friend had asked me about how he can move docker image without the base layer such that the shipping size of the docker images can be reduced significantly.</p> <p>Below I list down some of the steps I followed along the way to achieve this.</p>","tags":["Docker"]},{"location":"blog/2021/09/08/#create-dockerfile","title":"Create Dockerfile","text":"<pre><code>FROM alpine:latest\n\nWORKDIR /workdir\n\nRUN echo \"Testing removing image layers\" &gt; notes.txt\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#build-the-docker-image","title":"Build the docker image","text":"<pre><code>\u276f docker buildx build -t jijo:1.0 -f Dockerfile .\n[+] Building 18.8s (7/7) FINISHED\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                   0.1s\n =&gt; =&gt; transferring dockerfile: 129B                                                                                                                                                   0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                      0.1s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                        0.0s\n =&gt; [internal] load metadata for docker.io/library/alpine:latest                                                                                                                      18.5s\n =&gt; [1/3] FROM docker.io/library/alpine:latest@sha256:eb3e4e175ba6d212ba1d6e04fc0782916c08e1c9d7b45892e9796141b1d379ae                                                                 0.0s\n =&gt; =&gt; resolve docker.io/library/alpine:latest@sha256:eb3e4e175ba6d212ba1d6e04fc0782916c08e1c9d7b45892e9796141b1d379ae                                                                 0.0s\n =&gt; CACHED [2/3] WORKDIR /workdir                                                                                                                                                      0.0s\n =&gt; CACHED [3/3] RUN echo \"Testing removing image layers\" &gt; notes.txt                                                                                                                  0.0s\n =&gt; exporting to image                                                                                                                                                                 0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                                0.0s\n =&gt; =&gt; writing image sha256:d1f3300e3448ef5eeb72cf95db5aebff3c8e740e69072bc23b29100bd5311509                                                                                           0.0s\n =&gt; =&gt; naming to docker.io/library/jijo:1.0\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#save-the-docker-image-to-local","title":"Save the docker image to local","text":"<p>Save the docker image to local and untar the file</p> <pre><code>docker save jijo:1.0 | gzip &gt; jijo.tar.gz\n</code></pre> <pre><code>mkdir -p jijo &amp;&amp; tar -xf jijo.tar.gz -C jijo --strip-components 1\n</code></pre> <pre><code>\u276f ll\ntotal 2.7M\n-rw-r--r-- 1 jijo jijo 92 Aug 24 22:15 Dockerfile\ndrwxr-xr-x 5 jijo jijo 4.0K Aug 25 09:30 jijo\n-rw-r--r-- 1 jijo jijo 2.7M Aug 25 09:30 jijo.tar.gz\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#find-the-base-layer-from-manifestjson","title":"Find the base layer from manifest.json","text":"<p>Analyze the contents of manifest.json, the first layer is the base layer. We will remove that folder and zip the file contents again </p> <pre><code>\u276f cd jijo &amp;&amp; cat manifest.json\n[{\n    \"Config\": \"d1f3300e3448ef5eeb72cf95db5aebff3c8e740e69072bc23b29100bd5311509.json\",\n    \"RepoTags\": [\"jijo:1.0\"],\n    \"Layers\": [\"11cbe68173689fb732863a26e9c9217da15b278edc951dcae1effb426247f521/layer.tar\", \"f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/layer.tar\", \"34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/layer.tar\"]\n}]\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#remove-the-base-layer-folder","title":"Remove the base layer folder","text":"<p>Finding first layer</p> <pre><code>FOLDER_TO_REMOVE=$(jq '.[].Layers[0]' manifest.json | sed s/\\\"//g  | sed s/\\\\/layer.tar//g)\nrm -r $FOLDER_TO_REMOVE\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#zip-the-contents-again","title":"Zip the contents again","text":"<pre><code>\u276f tar -zcvf jijo.tar.gz .\n./\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/VERSION\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/json\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/layer.tar\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/VERSION\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/json\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/layer.tar\n./d1f3300e3448ef5eeb72cf95db5aebff3c8e740e69072bc23b29100bd5311509.json\n./repositories\n./manifest.json\ntar: .: file changed as we read it\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#peek-the-contents-of-tar-file","title":"Peek the contents of tar file","text":"<pre><code>\u276f tar -tf jijo.tar.gz\n./\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/VERSION\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/json\n./34a8e8adfac3eb497b3683326e980a62a322094bf8ed85be3c46d73a3cd209e3/layer.tar\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/VERSION\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/json\n./f65769e67f610c4c0784eb7585bfcbb53d5da988f057363265d7e72675580443/layer.tar\n./d1f3300e3448ef5eeb72cf95db5aebff3c8e740e69072bc23b29100bd5311509.json\n./repositories\n./manifest.json\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#clear-the-cache-and-remove-the-base-image","title":"Clear the cache and remove the base image","text":"<p>Clear the cache and remove the base image in order to confirm that the docker image doesn't load from tar file without the base image</p> <pre><code>\u276f docker rmi jijo:1.0\nUntagged: jijo:1.0\nDeleted: sha256:d1f3300e3448ef5eeb72cf95db5aebff3c8e740e69072bc23b29100bd5311509\n\n\u276f docker rmi alpine:latest\nError: No such image: alpine:latest\n\n\u276f docker system prune\nWARNING! This will remove:\n  - all stopped containers\n  - all networks not used by at least one container\n  - all dangling images\n  - all dangling build cache\n\nAre you sure you want to continue? [y/N] y\nDeleted Containers:\n1df25e5e691b9a3a2d51580b9f44ce9da1f2640aff0d5122bc81d1c85b4a0968\need8a6031062c25da5e3c32ee6ecfc9f9af7de01f3fef7e69fd9ea19f1839319\n9af26acfc97bb38f3bff4bab7a51359c1e1877fa38a427f5647a6c5fb83fed7e\n1a69feef5b3e99ac25ca0038ddc51107c8e7ffcfd32bc89796cf0da741629042\n21d621a952d4835d3eab684e2864d25428e3ba329e02e4ac0d1ea14e1fdb2ee9\n71522ba0ed8714c04297d58c72ef1b6d169f8d0f26ee732e027df301195ce67b\n4e9d4aa31c777027cd84d710c0206e155a0428f5f6b668627bc03aa7bb812b9f\n9f9ca76b10f4991a3ebd7f1dfa44084797ad9b42d2d3a0fa28187fb0d894b1d5\nf08764ce7d21f5b1f5f2b8243e1c77a03455af71268fe3e4685824d9628af1b1\n8cc554e54f6b5da73c6273406d9389b664fb84e9b73a16f13f8f260897fa817d\n21201d6a53b01d14ec2d588b3a2230fbfb0bc06bce048af2d93099b520873fb7\naddf3a8c8ea67b077c1f3f25a6281f77f19a378fe6932afae73b967e9e6adf53\n665e592899a8bd7f6feae2a43f56efc3678e898d4e7b506fc113e69edf801759\n4a6ce095a4de054e0542bd87a272a523ea85ae7afd5b8a4969fc44deabfb2122\n21863ed7a45616c985411af96d709a13ed5c864b1f508fc76a8e9c767acb691d\nc0d0179d675efc9655f27ba181736afd6184bf279e4e150f0430b836245935b2\nf44631dd1e20e90ad18d9683f5e4de407da1c72eb94dad441146d3d8a6ed1414\n587648e36694705ab401c32deb290142b3d58813443c768f5c79454e0ce56438\ndf3770c68de6bd787cb1e29a30499075f12a7818db50ba28b01322e948361d9e\n7889331b1588c03db54c0ec4f0d653811be2f26ff844a56efdefc7e5cb704733\n55e09a0770e480e3828ac1a1e662b76e3dca1c7cec56d917aee849554fb088ac\n\nDeleted build cache objects:\nxeopt73hh8934sras1f2lenkp\nfoju7uafxvy2pisuy6yzjvbw1\n\nTotal reclaimed space: 34.05kB\n\n\u276f docker buildx prune\nWARNING! This will remove all dangling build cache. Are you sure you want to continue? [y/N] y\nTotal:  0B\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#first-try-load-the-docker-image","title":"First try : Load the docker image","text":"<pre><code>\u276f docker load -i jijo.tar.gz\nopen /var/lib/docker/tmp/docker-import-816606677/11cbe68173689fb732863a26e9c9217da15b278edc951dcae1effb426247f521/layer.tar: no such file or directory\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#second-try-load-the-docker-image","title":"Second try: Load the docker image","text":"<pre><code>\u276f docker pull alpine:latest\nlatest: Pulling from library/alpine\n29291e31a76a: Pull complete\nDigest: sha256:eb3e4e175ba6d212ba1d6e04fc0782916c08e1c9d7b45892e9796141b1d379ae\nStatus: Downloaded newer image for alpine:latest\ndocker.io/library/alpine:latest\n\n\u276f docker load -i jijo.tar.gz                                                                                                                         \uf252 10s\n17938a3475fd: Loading layer [==================================================&gt;]  2.048kB/2.048kB\n506cf94d07ed: Loading layer [==================================================&gt;]  3.072kB/3.072kB\nLoaded image: jijo:1.0\n</code></pre>","tags":["Docker"]},{"location":"blog/2021/09/08/#success","title":"Success !! \ud83d\udd7a","text":"","tags":["Docker"]},{"location":"blog/2022/02/03/","title":"Understanding Container Image Security with Anchore and Trivy","text":"","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#understanding-container-image-security-with-anchore-and-trivy","title":"Understanding Container Image Security with Anchore and Trivy","text":"<p>In today's world of containerized applications, ensuring the security of container images is paramount. Two powerful tools that aid in this process are Anchore and Trivy. This blog post delves into what these tools are and how they can be used to analyze and secure container images.</p>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#what-is-anchore","title":"What is Anchore?","text":"<p>Anchore Engine is an open-source project that provides a centralized service for the inspection, analysis, and certification of container images. It is available as a Docker container image that can be run standalone or within orchestration platforms such as Kubernetes, Docker Swarm, Rancher, Amazon ECS, and others.</p>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#summarizing-vulnerabilities-with-anchore","title":"Summarizing Vulnerabilities with Anchore","text":"<p>To process Anchore vulnerability reports and get a summary of vulnerabilities, you can use the following script:</p> <pre><code>for i in *-vuln.json; do\n    echo $i &gt;&gt; scan.log\n    jq -r '.vulnerabilities[].severity' $i | sort | uniq -c &gt;&gt; scan.log\ndone\n</code></pre>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#what-is-trivy","title":"What is Trivy?","text":"<p>Trivy (pronounced \"tri\" like trigger and \"vy\" like envy) is a simple and comprehensive scanner for vulnerabilities in container images, file systems, and Git repositories, as well as for configuration issues. Trivy detects vulnerabilities in OS packages (e.g., Alpine, RHEL, CentOS) and language-specific packages (e.g., Bundler, Composer, npm, yarn). Additionally, Trivy scans Infrastructure as Code (IaC) files such as Terraform, Dockerfile, and Kubernetes manifests to detect potential configuration issues that could expose your deployments to risk.</p>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#generating-a-csv-summary-with-trivy","title":"Generating a CSV Summary with Trivy","text":"<p>To process Trivy JSON reports and generate a summary in CSV format, you can use the following script:</p> <pre><code>for FILE in *.json; do\n    jq --compact-output --raw-output '.[] | select(.Vulnerabilities) | {type:.Type, class:.Class, vulnerability:.Vulnerabilities[]} | [.type, .class, .vulnerability.VulnerabilityID, .vulnerability.Severity, .vulnerability.PublishedDate, .vulnerability.LastModifiedDate] | @csv' ${FILE} | awk -v NAME=${FILE} '{print NAME, $0}'\ndone &gt; output.log\n</code></pre> <p>The final output can be found in <code>output.log</code>.</p>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2022/02/03/#references","title":"References","text":"<ol> <li>Anchore Engine GitHub Repository</li> <li>Trivy GitHub Repository</li> </ol> <p>By leveraging Anchore and Trivy, you can enhance the security of your containerized applications, ensuring that vulnerabilities are identified and addressed promptly.</p>","tags":["Anchore","Trivy","Docker","Vulnerability"]},{"location":"blog/2023/08/16/","title":"Asking Notion AI to write a tutorial on K8s","text":"<p>This might seem trivial right now on why I decided to write this as there are many tutorials and official documentation which are better. But I wanted to try Notion's AI capability and this is the result that it gave when I asked it to write a tutorial on Kubernetes. Pretty amazed by the accuracy and how concise it is. :)</p> <p>Kubernetes is an open-source platform that automates container deployment, scaling, and management. It is an increasingly popular tool for managing containerized applications, and getting started with Kubernetes on your laptop is a great way to learn its basic concepts and features.</p>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/08/16/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following installed on your laptop:</p> <ul> <li>Docker</li> <li>Kubernetes CLI (kubectl)</li> <li>A Kubernetes cluster (e.g., minikube)</li> </ul>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/08/16/#step-1-start-a-kubernetes-cluster","title":"Step 1: Start a Kubernetes Cluster","text":"<p>To start a Kubernetes cluster on your laptop, you can use a tool like minikube. Minikube is a lightweight Kubernetes implementation that creates a single-node cluster on your local machine.</p> <p>To start a minikube cluster, run the following command in your terminal:</p> <pre><code>minikube start\n</code></pre> <p>This will start a single-node Kubernetes cluster on your laptop.</p>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/08/16/#step-2-deploy-an-application","title":"Step 2: Deploy an Application","text":"<p>Once you have a Kubernetes cluster up and running, you can deploy your first application. In Kubernetes, applications are defined in YAML files called manifests. A manifest describes the desired state of your application, including the container image to use, how many replicas to run, and what services to expose.</p> <p>Here's an example manifest for a simple \"Hello, World!\" application:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello-world\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: hello-world\n        image: docker.io/library/hello-world:latest\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: hello-world\n</code></pre> <p>Save this manifest to a file (e.g., <code>hello-world.yaml</code>) and deploy it to your Kubernetes cluster by running the following command:</p> <pre><code>kubectl apply -f hello-world.yaml\n</code></pre> <p>This will create a new deployment and service for the \"Hello, World!\" application.</p>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/08/16/#step-3-verify-your-application-is-running","title":"Step 3: Verify Your Application is Running","text":"<p>To verify that your application is running, you can use the <code>kubectl get</code> command to list the pods and services in your Kubernetes cluster:</p> <pre><code>kubectl get pods\nkubectl get services\n</code></pre> <p>This will show you the status of your application and its associated services.</p>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/08/16/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've deployed your first application on a Kubernetes cluster running on your laptop. This is just the beginning of what you can do with Kubernetes, and there's a lot more to learn. But with these basic steps, you're well on your way to becoming a Kubernetes pro.</p>","tags":["AI","K8s","Tutorial"]},{"location":"blog/2023/11/07/","title":"Troubleshooting Tkinter in WSL2","text":"<p>If you have attempted various solutions from Stack Overflow and Tkinter still does not function correctly in WSL2, this guide aims to assist you.</p>","tags":["Python","WSL2"]},{"location":"blog/2023/11/07/#the-issue","title":"The Issue","text":"<p>Despite installing the <code>tcl/tk</code> library in WSL2, Tkinter may not work with Python 3.11. This was the issue encountered. Multiple fixes were attempted without success, although it had previously worked, prompting further investigation into the cause.</p>","tags":["Python","WSL2"]},{"location":"blog/2023/11/07/#the-solution","title":"The Solution","text":"<p>Downgrading Python to version 3.8 resolved the issue. Additionally, it may be necessary to install the Tk libraries using the following command:</p> <pre><code>sudo apt install python3-tk tk-dev\n</code></pre>","tags":["Python","WSL2"]},{"location":"blog/2023/11/07/#conclusion","title":"Conclusion","text":"<p>Downgrading Python to version 3.8 and installing the required Tk libraries should address the issue.</p> <p>Thank you for reading!</p>","tags":["Python","WSL2"]},{"location":"tags/","title":"Tags","text":"<p>Follow the tags</p>"},{"location":"tags/#ai","title":"AI","text":"<ul> <li>Asking Notion AI to write a tutorial on K8s</li> </ul>"},{"location":"tags/#anchore","title":"Anchore","text":"<ul> <li>Understanding Container Image Security with Anchore and Trivy</li> </ul>"},{"location":"tags/#apache-sqoop","title":"Apache Sqoop","text":"<ul> <li>Sqoop Views in Netezza to HDFS</li> </ul>"},{"location":"tags/#bigdata","title":"Bigdata","text":"<ul> <li>Sqoop Views in Netezza to HDFS</li> </ul>"},{"location":"tags/#centos","title":"CentOS","text":"<ul> <li>Create sudo user in CentOS</li> <li>Install RStudio in CentOS 6</li> </ul>"},{"location":"tags/#cloudera","title":"Cloudera","text":"<ul> <li>Setup HBase for Cloudera</li> </ul>"},{"location":"tags/#docker","title":"Docker","text":"<ul> <li>Moving Docker images without base image</li> <li>Understanding Container Image Security with Anchore and Trivy</li> </ul>"},{"location":"tags/#go","title":"Go","text":"<ul> <li>GO KIT vs Mux vs net/http</li> </ul>"},{"location":"tags/#gokit","title":"Gokit","text":"<ul> <li>GO KIT vs Mux vs net/http</li> </ul>"},{"location":"tags/#hbase","title":"HBase","text":"<ul> <li>Setup HBase for Cloudera</li> </ul>"},{"location":"tags/#k8s","title":"K8s","text":"<ul> <li>Asking Notion AI to write a tutorial on K8s</li> </ul>"},{"location":"tags/#kubernetes","title":"Kubernetes","text":"<ul> <li>Setup SparkOperator and Minio in K8s</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Create sudo user in CentOS</li> </ul>"},{"location":"tags/#mux","title":"Mux","text":"<ul> <li>GO KIT vs Mux vs net/http</li> </ul>"},{"location":"tags/#netezza","title":"Netezza","text":"<ul> <li>Sqoop Views in Netezza to HDFS</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>Troubleshooting Tkinter in WSL2</li> </ul>"},{"location":"tags/#rstudio","title":"RStudio","text":"<ul> <li>Install RStudio in CentOS 6</li> </ul>"},{"location":"tags/#spark","title":"Spark","text":"<ul> <li>Setup SparkOperator and Minio in K8s</li> </ul>"},{"location":"tags/#sparkoperator","title":"SparkOperator","text":"<ul> <li>Setup SparkOperator and Minio in K8s</li> </ul>"},{"location":"tags/#trivy","title":"Trivy","text":"<ul> <li>Understanding Container Image Security with Anchore and Trivy</li> </ul>"},{"location":"tags/#tutorial","title":"Tutorial","text":"<ul> <li>Asking Notion AI to write a tutorial on K8s</li> </ul>"},{"location":"tags/#vulnerability","title":"Vulnerability","text":"<ul> <li>Understanding Container Image Security with Anchore and Trivy</li> </ul>"},{"location":"tags/#wsl2","title":"WSL2","text":"<ul> <li>Troubleshooting Tkinter in WSL2</li> </ul>"},{"location":"tags/#golang","title":"golang","text":"<ul> <li>GO KIT vs Mux vs net/http</li> </ul>"},{"location":"tags/#nethttp","title":"net/http","text":"<ul> <li>GO KIT vs Mux vs net/http</li> </ul>"}]}